
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Xizheng (Hugh) Yu is a junior student at the University of Wisconsin-Madison concentrating in Computer Sciences and Data Science. He is a highly motivated person who’s passionate about software development and open-source projects. He is generally interested in research related to Computer Vision and Machine Learning. He is a cooperative team player, a proactive communicator, and a persevering developer.\n","date":1609615045,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1609615045,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Xizheng (Hugh) Yu is a junior student at the University of Wisconsin-Madison concentrating in Computer Sciences and Data Science. He is a highly motivated person who’s passionate about software development and open-source projects.","tags":null,"title":"Xizheng (Hugh) Yu","type":"authors"},{"authors":null,"categories":null,"content":"Operating Systems 2022 Fall UW-Madison by Remzi H. Arpaci-Dusseaue\nTextbook: OSTEP by Remzi H. Arpaci-Dusseau\nOur Projects on Github: Operating-System-2022-Fall\n","date":1672358400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1672358400,"objectID":"d990fc61ead4068ce67fc7d482523ab5","permalink":"https://x12hengyu.github.io/notes/537os/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/537os/","section":"notes","summary":"Operating Systems 2022 Fall UW-Madison by Remzi H. Arpaci-Dusseaue\nTextbook: OSTEP by Remzi H. Arpaci-Dusseau\nOur Projects on Github: Operating-System-2022-Fall","tags":null,"title":"CS537: Introduction to Operating Systems","type":"book"},{"authors":null,"categories":null,"content":"Summer 2022 UW-Madison CS564 Database Management Systems: Design and Implementation\n","date":1672358400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1672358400,"objectID":"1836fc7b58f0c44edb275c2d5e5942ce","permalink":"https://x12hengyu.github.io/notes/564db/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/","section":"notes","summary":"Summer 2022 UW-Madison CS564 Database Management Systems: Design and Implementation","tags":null,"title":"CS564: Database Management Systems: Design and Implementation","type":"book"},{"authors":null,"categories":null,"content":"My D2l Learning Notes Resources Access the book through this link Access video lecture (Chinese) at Bilibili, or YouTube. Github ipynb Code Repository. ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"24dede1f640b28066a0bf653da7ca94d","permalink":"https://x12hengyu.github.io/notes/d2l/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/","section":"notes","summary":"My D2l Learning Notes Resources Access the book through this link Access video lecture (Chinese) at Bilibili, or YouTube. Github ipynb Code Repository. ","tags":null,"title":"Dive into Deep Learning","type":"book"},{"authors":null,"categories":null,"content":" Entities represented by rectangles. Attributes represented by circles/ellipses. Key attributes are underlined. Relationships represented by diamonds Cardinality represented by arrows Lines link attributes to entity sets and entity sets to relationship sets. PARTICIPATION CONSTRAINT\nA participation constraint defines how every member of entity set participates in the relationship.\nTotal participation:\nEvery member of entity set must participate in the relationship\nRepresented by bold line from entity rectangle to relationship diamond\n","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"2b5ef498a9a63ca2305b9d782b06ec72","permalink":"https://x12hengyu.github.io/notes/564db/1/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/1/","section":"notes","summary":"Entities represented by rectangles. Attributes represented by circles/ellipses. Key attributes are underlined. Relationships represented by diamonds Cardinality represented by arrows Lines link attributes to entity sets and entity sets to relationship sets.","tags":null,"title":"ER DIAGRAM","type":"book"},{"authors":null,"categories":null,"content":"VIRTUALIZATION: Build an illusion of unlimited hardware, e.g., as many CPUs as needed; a large, private memory region for each program.\nTime-Sharing: Multiple applications use the same resource at different times Share the same CPU core among multiple processes Space-Sharing: Multiple applications use parts of the same resource at the same time Allocate a certain number of CPU (cores) to a process Goals:\nEfficiency Virtualization should not create a significant overhead Security Protect processes form each other Protect the OS from processes High-level Goal Give each running program the impression it alone is actively using the CPU and other resources Processes:\nPrograms vs. Processes: We Compile Programs and Run Processes We want each running program that impression it alone is actively using the CPU and other resources. Things changes during programs running: I/O: open file descriptors, write to disk, network, screen CPU: Registers, Program Counter, General Purpose Pause a process: On Pause: Process state is stored on its kernel stack. The kernel stack is managed by the OS among other process metadata On Resume: Process state is restored from kernel stack to the CPU Limited Direct Execution:\nProblems: Security: process might do restricted things Efficiency: process might do things slow Process may run forever Solution: User Mode (restricted mode): limited instructions, single address space Kernel Mode (privileged): execute all instructions, access all of memory Switching between user and Kernel Mode: Trap: Enters kernel mode from user mode Code invokes system calls Code performs illegal/restricted operation Return-from-trap: Executed after returning the process Trap handlers: Set up at boot time by the OS Executed after a process calls a trap Multi-Processes:\nSo far: Cooperative multi-tasking Can only switch to another task if the process says so Non-cooperative multi-tasking Time-share the same CPU core among multiple processes Switch between processes every few milliseconds Timer Interrupts Setup up by the operating system at boot time Timer interrupt the CPU every few milliseconds Operating System may run scheduler* to decide which process to run next Do not schedule/run a blocked processes\nThreads:\nThreads: “Lightweight process” A process can have multiple threads All threads of a process read/write the processes memory They are handled (almost) the same as processes by the OS Each thread has their own stack and kernel stack Threads can be ready, running, or blocking SCHEDULING: Minimize turnaround time (turnaround_time = completion_time – arrival_time)\nWant job to be completed as soon as possible FIFO (First In, First Out), run jobs in arrival order A: arrival 0, run 10, B: arrival 0, run 10, C: arrival 0, run 10 Average Turnaround Time = (10+20+30)/3 = 20s Problem: Convoy problem, A pretty long job A: arrival 0, run 100, B: arrival 0, run 10, C: arrival 0, run 10 Average Turnaround Time = (100+110+120)/3 = 110s SJF (Shortest Job First) Moving shorter job before longer job improves turnaround time of short job A: arrival 0, run 100, B: arrival 0, run 10, C: arrival 0, run 10 Average Turnaround Time = (10+20+120)/3 = 50s Problem: Jobs do not arrive at the same time A: arrival 0, run 100, B: arrival 10, run 10, C: arrival 10, run 10 Average Turnaround Time = (100+110-10+120-10)/3 = 103s STCF (Shortest Time-to-Completion First) Always run job that will complete the quickest A: arrival 0, run 100, B: arrival 10, run 10, C: arrival 10, run 10 Average Turnaround Time = (10+20+120)/3 = 50s Problem: what if we do not know job runtime? Minimize response time (response_time = first_run_time – arrival_time)\nCan’t control how long job needs to run; minimize time before scheduled Round-Robin (RR) Alternate ready processes for a fixed-length time-slice, Short jobs will finish after fewer time-slices Features: May increase turnaround time, decreases response time Potentially causes additional context switches A: arrival 0, run 5, B: arrival 0, run 5, C: arrival 0, run 5 Average Turnaround Time = (13+14+15)/3 = 14s Average Response Time = (1-0+2-0+3-0)/3 = 2s MLFQ (Multi-Level Feedback Queue) Support two job types Interactive programs care about response time Batch programs care about turnaround time Approach: Multiple levels of round-robin Each level has higher priority than lower level Can preempt them Rules: If priority(A) \u0026gt; Priority(B), A runs If priority(A) == Priority(B), A \u0026amp; B run in Round-Robin Processes start at top priority If job uses whole slice, demote process Starvation Problem: Low priority batch job may never get scheduled if other jobs at high priority Solution: Periodically boost priority of all jobs (or all jobs that haven’t been scheduled) Problem: Job could trick scheduler by not using entire time-slice, (doing I/O just before time-slice end) Lottery Scheduling Approach: give processes lottery tickets whoever wins runs higher priority =\u0026gt; more tickets Maximize throughput (jobs completed / second) …","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"7e273c57c396b23717e593aed06057fd","permalink":"https://x12hengyu.github.io/notes/537os/1/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/537os/1/","section":"notes","summary":"VIRTUALIZATION: Build an illusion of unlimited hardware, e.g., as many CPUs as needed; a large, private memory region for each program.\nTime-Sharing: Multiple applications use the same resource at different times Share the same CPU core among multiple processes Space-Sharing: Multiple applications use parts of the same resource at the same time Allocate a certain number of CPU (cores) to a process Goals:","tags":null,"title":"Midterm Review","type":"book"},{"authors":null,"categories":null,"content":"Get Started import torch x = torch.arange(12) # create vector of evenly spaced values, starting at 0 (included) and ending at n (not included). # tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) x.shape # torch.Size([12]) x.numel() # number of elements # 12 X = x.reshape(3, 4) # reshape vector # tensor([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) # -1 inferred automatically, we could have equivalently called x.reshape(-1, 4) # or x.reshape(3, -1) torch.zeros((2, 3, 4)) # construct a tensor with all elements set to zero and a shape of (2, 3, 4) torch.ones((2, 3, 4)) # construct a tensor with all elements set to one and a shape of (2, 3, 4) torch.tensor([[1, 2], [3, 4], [5, 6]) # use nested list to create tensor Indexing and Slicing X[start:stop] - start is inclusive, stop is exclusive X[-1] # last row X[1:3] # second and third rows X[:, 0] # first col X[1,2] = 9 # assign 9 to position [1, 2] X[0:2, :] = 12 # assign 12 to row 0, 1 Operations The common standard arithmetic operators for addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**) have all been lifted to elementwise operations for identically-shaped tensors of arbitrary shape. x = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y torch.cat((X, Y), dim=0) - concatenate tensors together\ndim=0 - 0 is the outer most layer of tensor (simple stack together), 1 will be 1 inner layer inside tensor. X == Y - tensor compare, return a same shape tensor with True (1) or False (0) elements\nX.sum() - sum tensor elements\ntorch.exp(x) - exponential e ** x\nBroadcasting - in this case, a duplicates column to (3, 2), b duplicates rows to (3, 2), then add.\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) # tensor([[0], [1], [2]]), tensor([[0, 1]]) a + b # tensor([[0, 1], # [1, 2], # [2, 3]]) A = X.numpy() - transfer from tensor to ndarray\nB = torch.tensor(A) - transfer from ndarray to tensor\na = torch.tensor([1]) - single element can transfer to python standard integer a.item(a) int(a) float(a) - 1, 1, 1.0 Memory before = id(Y) Y = Y + X id(Y) == before # false, Y id different Z = torch.zeros_like(Y) Z[:] = X + Y # Z id does not change Y[:] = Y + X id(Y) == before # true, Y id same before = id(X) X += Y id(X) == before # true, X id same, this is same as previous one Data import os os.makedirs(os.path.join(’..’,’data’)) - create directory exist_ok = True - whether overwrite os.path.join() - join names with / to construct correct path open(data, ‘w’) - write mode to file import pandas as pd df = pd.read_csv(path) df.iloc[row, col] - index dataframe df.fillna(df.mean()) - replace NA val with mean pd.get_dummies(df, dummy_na = True) - Turn NaN to a column of feature, like onehot torch.tensor(df.values) - turn df to tensor, automatically use float 64, but slow, we generally change to float 32 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7dc4f26b17d972f1c3a24c31da01d7ab","permalink":"https://x12hengyu.github.io/notes/d2l/1/1-data-manipulation-47bd47c1eef842699a100da3eaaa5fcb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/1/1-data-manipulation-47bd47c1eef842699a100da3eaaa5fcb/","section":"notes","summary":"Get Started import torch x = torch.arange(12) # create vector of evenly spaced values, starting at 0 (included) and ending at n (not included). # tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) x.","tags":null,"title":"1. Data Manipulation","type":"book"},{"authors":null,"categories":null,"content":"Deadlock: Deadlocks can only happen when these four conditions hold:\nmutual exclusion\nProblem: Threads claim exclusive control of resources that they require\nStrategy: Eliminate locks!\nhold-and-wait\nProblem: Threads hold resources while waiting for additional resources\nStrategy: Acquire all locks atomically, Can release locks over time, but cannot acquire again until all have been released\nHow: Use a meta lock\nDrawbacks:\nMust know ahead of time which locks will be needed Must be conservative (acquire any lock possibly needed) Degenerates to just having one big lock (reduces concurrency) no preemption\nProblem: Resources (e.g., locks) cannot be forcibly removed from other threads\nStrategy: if thread can not get what it wants, release what it holds\nDrawbacks:\nPotential Livelock: No processes make progress, but state of involved processes constantly changes Classic solution: Exponential random back-off circular wait\nCircular chain that each thread holds a resource (e.g., lock) requested by next thread in chain\nExample: Lock Ordering in Xv6\na lock on the directory a lock on the new file’s inode a lock on a disk block buffer idelock ptable.lock Practical Solution:\nDecide which locks must be acquired before others If A before B, never acquire A if B is already held! Document and write code accordingly Concurrency:\nPersistence:\nI/O caption Interface: present to the rest of the system, and system software can control the operations.\nInternal: implement the abstraction presented.\nStatus: read to see the current status of the device;\nCommand: tell the device to perform a certain task;\nData: pass data to the device, or get data from the device.\nSteps: polling for status ready→OS send data→OS send command→polling for status finished\nApproach 1: Special I/O instructions\nAddition to the CPU’s instruction set that allows accessing hardware e.g, IN and OUT in x86 Approach 2: Memory-Mapped I/O\nDevice registers are mapped into memory OS can write to device registers like to any other memory location Not the same as mmap! Problems\nPolling waste CPU time.\nData movement is very CPU intensive\nSolution 1: SPIN-free Device Access\nInstead of Polling, go to sleep Process state changes to BLOCKED Device notifies when it is done using an interrupt Problems: If device is very fast, we get very frequent interrupts Leads to context switch overhead Solution 2: DMA (Direct Memory Access)\nOS telling DMA engine where the data in memory, how much data to copy, and which device to send. At that point, the OS is done with the transfer and can proceed with other work.\nDMA is complete raise an interrupt.\nFaster than copying to CPU and then to disk CPU can do other things while data is being moved Requires specialized hardware DISK\nPlatter:\n2-sided, A drive can have multiple platters, read or written Rotates track at fixed speeds Track:\nSplit into fixed-size sectors (often 512 bytes), Contain redundant encoding to recover from corrupted bits exposed to the OS as a linear array Spindle: spins the platter around at a constant fixed rate with unit Rotations Per Minute.\nDisk Arm + Read/write Head: Moves between tracks of the platter\nController:\nexecutes operations stored in command buffer Writes output to status or data registers translates track and sector location accesses to disk into internal actions track of multiple actions at once Access speed\nseek: move disk arm to correct track\nwait (rotation): wait for sector to rotate under arm\ntransfer: read/write data\ne.g. Read one sector (512B) avg. seek: 7ms avg. rotate: 3ms avg. transfer: ~0ms (200Mb/s) throughput is 512b/10ms ≈ 50kb/s\nSlow!!!\nRandom I/O are dominated by seek and rotation Sequential accesses can be much faster Disk Scheduling\nGoal: Maximize throughput by minimizing seek and rotation times\nDuration of each access is known FIFO (First-in-first-out) Shortest-Seek First (SSTF): To maximize throughput pick next operation with smallest seek time, Can be implemented by the OS as nearest block first Does not account for rotation Disk arm might stay on same track for a long time Some operations could starve SCAN or “Elevator”, F-SCAN for “Freeze”, C-SCAN for “Circular” Does not take rotation time into account, only seek Better starvation free “Shortest-Job First” algorithms exist Shortest Positioning/Access Time First (SPTF/SATF): both seek and rotate accounted. JBOD: Just a Bunch Of Disks\nApplication stores different files on different file systems\nDisadvantages:\nApplication must manage multiple devices Not portable across different system configurations RAID: Redundant Array of Inexpensive Disks\ntransparent and easy to deploy Logical disk gives capacity, performance, and reliability Economies of scale RAID Mapping Dynamic mapping (page table approach) Logical x sometimes maps to physical y and sometimes z Use data structure (array, hash table, tree) Static mapping (RAID approach) Logical x always maps to physical Uses simple math; avoids extra look-ups Redundancy:\nIncrease number of copies: …","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"32058700311024555d80b9ccc5f80dd8","permalink":"https://x12hengyu.github.io/notes/537os/2/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/537os/2/","section":"notes","summary":"Deadlock: Deadlocks can only happen when these four conditions hold:\nmutual exclusion\nProblem: Threads claim exclusive control of resources that they require\nStrategy: Eliminate locks!\nhold-and-wait\nProblem: Threads hold resources while waiting for additional resources","tags":null,"title":"Final Review","type":"book"},{"authors":null,"categories":null,"content":"ARMSTRONG’S AXIOMS\nReflexivity rule: X → a subset of X Augmentation rule: X → Y, then XZ → YZ Transitivity rule: X → Y and Y → Z then X→Z Union rule: X→Y and X→Z, then X→YZ (X, Y, Z are sets of attributes) Decomposition rule: X → YZ, then X → Y and X → Z Pseudo-transitivity rule: X → Y and YZ → U, then XZ → U DEFINITIONS\nClosure: $S^+$ = all FDs logically implied by S\nSimple attribute: An attribute that cannot be further subdivided into components is a simple attribute.\nComposite attribute: An attribute that can be split into components is a composite attribute.\nNon-prime attribute: An attribute that is not part of any candidate key.\nPrime attribute: An attribute that is a part of one of the candidate keys.\nTrivial functional dependency: If X → Y and Y is the subset of X,\ne.g. AB → B, B is subset of AB; A → A, A is subset of A\nNon-Trivial functional dependency: If X → Y and Y is not a subset of X,\ne.g. AB → C, C is not subset of AB\nPartial functional dependency: the non-prime attribute is functionally dependent on part of a candidate key.\ne.g. AC → B, A → D, D → B; [A]+ = {A, B, D} then B is partially dependent on AC\nTransitive Functional Dependency: dependent is indirectly dependent on determinant\ne.g. If A → B and B → C, then A → C is Transitive FD\nMultivalued Functional Dependency: entities of the dependent set are not dependent on each other.\ne.g. If A → BC and there exists no FD between B and C\nCandidate Key \u0026amp; Super Key:\nCandidate Key:\nfunctionally determines all attributes of R none of its subsets determines all attributes of R Super Key:\nset of attributes that contains a candidate key Properties:\nA Candidate Key is a minimum SuperKey\nA Candidate Key is a SuperKey\nSteps to find Key Attributes:\nR(A, B, C, D, E), FD = {A → B, BC → E, DE → A}\nBrowse attributes on the right hand of each FD, {B, E, A} are already functionally depended. Then rest attributes {C, D} are key attributes. With Key attributes, we can check each combination to find candidate keys. [CD]+ = {C, D} → not key [ACD]+ = {A, B, C, D, E} → candidate key [BCD]+ = {B, C, D, E, A} → candidate key [CDE]+ = {C, D, E, A, B} → candidate key Example:\nR(A, B, C, D, E), FD = {A → B, BC → E, DE → A}\nC, D will be key attributes since they are not depended on any attributes.\n[CD]+ = {C, D} → not key\n[ACD]+ = [BCD]+ = [CDE]+ = {A, B, C, D, E} → candidate keys are found\nCandidate keys: ACD, BCD, CDE\nSuper Keys (must include CD, then find all combinations):\nACD, BCD, CDE, ABCD, BCDE, ACDE, ABCDE\nNORMAL FORMS\nFirst Normal Form (1NF):\nEvery field must contain atomic values (i.e no sets or lists).\nNo null values.\nHas unique key.\nAll relations are in 1NF.\nExample:\nNot Atomic: R(Name, Course): (steve, c/c++)\nAtomic: R(Name, Course): (steve, c), (steve, c++)\nSecond Normal Form (2NF):\nThe table should be in the First Normal Form (1NF). There should be No Partial Dependency for non-prime attributes. Lemmas:\nR is in 2NF if it does not contain any composite key (implies no partial dependency) and is in 1NF\nExample:\nR(A, B, C, D), FD = {AC → B, A → D, D → B}\nWe can compute the closure of [A]+ = {A, D, B}\nTherefore, A is alone capable of determining B, which means B is Partially Depends on AC.\nTo decompose R and FD, we could take B out of R\nR1(A, B, C), FD = {AC → B}\nR2(A, B, D), FD = {A → D, D → B)\nThen there is no violation in both R1 and R2.\nThird Normal Form (3NF):\nThe table should be in the Second Normal Form (2NF). No Transitive Dependency for non-prime attributes Lemmas (for quick identification): at least one of the following must be true\nX → Y is a Trivial FD X is SuperKey for R Y is part of a candidate key Advantages:\n3NF is considered adequate for normal relational database design most of the 3NF tables are free of insertion, update, and deletion anomalies. 3NF always ensures functional dependency preserving and lossless Example:\nR(A, B, C, D, E), FD = {A → B, A → C, C → D, A → E}\nSince A is missing on the right side of FDs, A is a key attribute.\n[A]+ = {A, B, C, D, E}, then A is candidate key.\nHowever, A → C \u0026amp; C → D implies A → D, then D is transitively dependent on A\nTo decompose to R to 3NF, Take D out of R,\nR1(C, D), FD = {C → D}\nR2(A, B, C, E), FD = {A → B, A → C, A → E}\nThis decomposition also in BCNF\nBoyce-Codd Normal Form (BCNF) or (3.5NF):\nThe table should be in the Third Normal Form (3NF). For every one of its dependencies X → Y, at least one of the following conditions hold X → Y is a trivial FD X is superkey of R Other Lemmas:\nAny 2-attribute relation is in BCNF.\nBCNF decomposition may always not possible with dependency preserving, however, it always satisfies lossless join condition\nExample:\nR(A, B, C, D, E), FD = {AB → CD, D → E, A → C, B → D}\n[AB]+ = {A, B, C, D, E}, then AB is candidate key\nD → E Violation: Transitive Dependency; D is not a Super key\nA → C Violation: Partial Dependency; A is not a Super key\nB → D Violation: Partial Dependency; B is not a Super key\nresolve D → E:\nR1(D, E), FD = {D → E}, (D is superkey)\nR2(A, B, C, D), FD = …","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"cf591230b40c913f70465d92d68737d1","permalink":"https://x12hengyu.github.io/notes/564db/2/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/2/","section":"notes","summary":"ARMSTRONG’S AXIOMS\nReflexivity rule: X → a subset of X Augmentation rule: X → Y, then XZ → YZ Transitivity rule: X → Y and Y → Z then X→Z Union rule: X→Y and X→Z, then X→YZ (X, Y, Z are sets of attributes) Decomposition rule: X → YZ, then X → Y and X → Z Pseudo-transitivity rule: X → Y and YZ → U, then XZ → U DEFINITIONS","tags":null,"title":"NORMAL FORMS","type":"book"},{"authors":null,"categories":null,"content":"Linear Algebra $|\\alpha \\mathbf{x}| = |\\alpha| |\\mathbf{x}|$\n$|\\mathbf{x} + \\mathbf{y}| \\leq |\\mathbf{x}| + |\\mathbf{y}|$\n$|\\mathbf{x}| \u0026gt; 0 \\text{ for all } \\mathbf{x} \\neq 0$\n$\\ell_p$ norm:\nFrobenius norm\nSymmetric: $\\mathbf{A} = \\mathbf{A}^\\top$\nGiven two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their dot product $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$) is a sum over the products of the elements at the same position: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$.\nLinear Algebra in PyTorch A = torch.arange(6).reshape(3, 2) # create a 3 x 2 matrix # (tensor([[0, 1], # [2, 3], # [4, 5]]) A.T # Transpose B = A.Clone() # Deepcopy, assign new memory space A * B # Hadamard Product a = 2 a + A # add 2 to all elements in A a * A # time 2 to all elements in A A = torch.arange(6 * 2).reshape(2, 3, 2) # tensor([[[ 0, 1], # [ 2, 3], # [ 4, 5]], # [[ 6, 7], # [ 8, 9], # [10, 11]]]) A.sum(axis=0) # reduction to first axis # tensor([[ 6, 8], # [10, 12], # [14, 16]] A.sum(axis=0).shape # torch.Size([3, 2]) A.sum(axis=1) # reduction to one more deeper axis, which is second axis # tensor([[ 6, 9], # [24, 27]] A.sum(axis=1).shape # torch.Size([2, 2])) A.sum(axis=[0,2]).shape # torch.Size([3])) A.mean() == (A.sum() / A.numel()) # average A.sum(axis=1, keepdims=True) # do not lose dimention A.cumsum() # cumulative sum Wrong Part: Axis: [1,2] → [4] Correct Result [1,2] → [2]\ntorch.dot(x, y) - Dot Product same as torch.sum(x * y) torch.mv(A, x) - matrix vector multiplication e.g. A.shape: [5, 4], x.shape: [4], result.shape: [5] torch.mm(A, B) - matrix matrix multiplication e.g. A.shape: [5, 4], B.shape: [4, 3], result.shape: [5, 3] torch.norm(vector/matrix) - F norm / L1 / L2 norm Calculus Derivative\nSlope\nSubderivative\nConversion between Vector and Scalar when calculating derivative\n$\\frac{\\partial y}{\\partial \\mathbf{x}}$ is a row vector\n$\\frac{\\partial \\mathbf{y}}{\\partial x}$ is a column vector\n$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ is a matrix\nGeneral\nTrick: Numerator does not change, inverse Denominator and append to the end e.g. $\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{x}}$: Y.shape: (m, l), x.shape: (n, 1), Y does not change, inverse x to (1, n), append to the end of Y, gives us shape (m, l, n). (omit 1) Chain Rule\nExample 1\nExample 2\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f19f70903ef1d6f9ba32db59461e3e48","permalink":"https://x12hengyu.github.io/notes/d2l/2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/2/","section":"notes","summary":"Linear Algebra $|\\alpha \\mathbf{x}| = |\\alpha| |\\mathbf{x}|$\n$|\\mathbf{x} + \\mathbf{y}| \\leq |\\mathbf{x}| + |\\mathbf{y}|$\n$|\\mathbf{x}| \u003e 0 \\text{ for all } \\mathbf{x} \\neq 0$\n$\\ell_p$ norm:\nFrobenius norm\nSymmetric: $\\mathbf{A} = \\mathbf{A}^\\top$","tags":null,"title":"2. Math: Linear Algebra, Calculus, Prob","type":"book"},{"authors":null,"categories":null,"content":"Data Definition Language (DDL): Define relational schemas Create/alter/delete tables and their attributes // Create table student // Student(snum:int, sname:String, level:String, major:String, age:integer) CREATE TABLE Student ( snum BIGINT, sname VARCHAR(50), major VARCHAR(50), level CHAR(2), age int, PRIMARY KEY (snum)); // Delete table enroll DROP TABLE enroll; // Add a foreign key constraint to table enroll ALTER TABLE enroll ADD FOREIGN KEY(snum) REFERENCES STUDENT(snum) ON UPDATE CASCADE; ELEMENTS OF TABLE DECLARATIONS\nALTER\n// add column ALTER TABLE table_name ADD column_name datatype; // drop column ALTER TABLE table_name DROP COLUMN column_name; e.g.\nVIEWS\na “virtual table,” a relation that is defined in terms of the contents of other tables and views. You may query a view as if it were a base table. CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name WHERE condition; e.g.\nStandard SQL requires these distinctions:\nThere can be only one PRIMARY KEY for a relation, but several UNIQUE attributes. No attribute of a PRIMARY KEY can ever be NULL in any tuple. But attributes declared UNIQUE may have NULL’s, and there may be several tuples with NULL**.** Two other declarations we can make for an attribute are:\nNOT NULL means that the value for this attribute may never be NULL. DEFAULT says that if there is no specific value known for this attribute’s component in some tuple, use the stated . Constraints:\nDEFAULT - Sets a default value for a column if no value is specified\ne.g.\nCREATE TABLE CLASS ( cname VARCHAR(255), meets_at VARCHAR(255), room VARCHAR(255) DEFAULT \u0026#39;R128’, fid bigint, PRIMARY KEY(cname)); NOT NULL - Ensures that a column cannot have a NULL value\nUNIQUE - Ensures that all values in a column are different\nPRIMARY KEY - A combination of a NOT NULL and UNIQUE. Uniquely identifies each row in a table\nFOREIGN KEY - Prevents actions that would destroy links between tables\nIf there is a foreign-key constraint from attributes of relation R to the primary key of relation S, two violations are possible:\nAn insert or update to R introduces values not found in S. A deletion or update to S causes some tuples of R to “dangle.” Suppose R and S, to handle nonexistent S must be rejected:\nDefault: Reject the modification.\nCascade: Make the same changes in R.\nDeleted Class: delete R tuple.\nUpdated Class: change value in R.\nSet NULL: Change S to NULL.\nWhen we declare a foreign key, we may choose policies SET NULL or CASCADE independently for deletions and updates: ON [UPDATE, DELETE][SET NULL CASCADE]\ne.g.\n// Create table ENROLL // Enroll (snum:integer, cname:String) CREATE TABLE ENROLL ( snum BIGINT, cname VARCHAR(255), FOREIGN KEY (snum) REFERENCES student(snum), FOREIGN KEY (cname) REFERENCES class(cname) on UPDATE CASCADE, primary key(snum, cname)); // FOREIGN KEY on ALTER TABLE ALTER TABLE Orders ADD FOREIGN KEY (PersonID) REFERENCES Persons(PersonID); CHECK - Ensures that the values in a column satisfies a specific condition\nCREATE INDEX - Used to create and retrieve data from the database very quickly\nData Manipulation Language (DML): ­Insert new tuples into relations ­Delete tuples from relations Modify various attributes of tuples Query the database INSRET INTO\n// 1. Specify both the column names and the values to be inserted: INSERT INTO table_name (column1, column2, column3, ...) VALUES (value1, value2, value3, ...); // 2. Adding values for all the columns does not need to specify the column names, // order of the values is in the same order as the columns: INSERT INTO table_name VALUES (value1, value2, value3, ...); // 3. intert many values INSERT INTO \u0026lt;relation\u0026gt; ( \u0026lt;subquery\u0026gt; ); // e.g. Insert a single tuple INSERT INTO student(snum,sname,level,major,age) VALUES(51135593,\u0026#39;Maria White’,\u0026#39;SR\u0026#39;,\u0026#39;English\u0026#39;,21); DELETE\nDELETE FROM table_name WHERE condition; DELETE FROM table_name; // Delete everything // e.g. Delete all tuples matching the a condition DELETE FROM student WHERE Age \u0026gt; 10 AND Age \u0026lt; 15; // e.g. Delete everything DELETE FROM Student; UPDATE\nUPDATE table_name SET column1 = value1, column2 = value2, ... WHERE condition; // e.g. Update all tuples matching a condition UPDATE student SET major = \u0026#39;International Study\u0026#39; WHERE snum = \u0026#39;51235593\u0026#39;; SELECT-FROM-WHERE\nSELECT column1, column2, ... AS name1, name2, ... FROM table_name WHERE condition; // select all the fields available in the table SELECT * FROM table_name; INSIDE WHERE\nAttribute names of the relations appearing in FROM clause\nComparison operators (=, \u0026lt;\u0026gt;, \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=)\nArithmetic operations (+, -, /, *)\nAND-OR-NOT\nWHERE condition1 AND condition2 AND condition3 ...; WHERE condition1 OR condition2 OR condition3 ...; WHERE NOT condition; // e.g. in where SELECT sname FROM Student WHERE Age \u0026gt; 20 AND Age \u0026lt; 30; LIKE - FIND PATTERNS IN STRINGS\nThe percent sign % represents zero, one, or multiple characters The underscore sign _ represents one, single character SELECT column1, column2, ... FROM table_name WHERE columnN LIKE pattern; …","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"7634eae9745a0b34b2075a9648805fc5","permalink":"https://x12hengyu.github.io/notes/564db/3/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/3/","section":"notes","summary":"Data Definition Language (DDL): Define relational schemas Create/alter/delete tables and their attributes // Create table student // Student(snum:int, sname:String, level:String, major:String, age:integer) CREATE TABLE Student ( snum BIGINT, sname VARCHAR(50), major VARCHAR(50), level CHAR(2), age int, PRIMARY KEY (snum)); // Delete table enroll DROP TABLE enroll; // Add a foreign key constraint to table enroll ALTER TABLE enroll ADD FOREIGN KEY(snum) REFERENCES STUDENT(snum) ON UPDATE CASCADE; ELEMENTS OF TABLE DECLARATIONS","tags":null,"title":"SQL","type":"book"},{"authors":null,"categories":null,"content":"Theories Computation Graph\nForward propagation\nMemory Complexity: O(1) Drawback: Gradient calculation for every layer, high time complexity Back propagation\nTime Complexity: O(N), Memory Complexity: O(N) Example\nLeft: Forward Propagation, Right, Back Propagation\nAutomatic Differentiation in PyTorch Calculate Gradient of $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\nx = torch.arange(4.0) x.requires_grad_(True) # or: x = torch.arange(4.0, requires_grad=True) x.grad # default is none y = 2 * torch.dot(x, x) # tensor(28., grad_fn=\u0026lt;MulBackward0\u0026gt;) y.backward() # Back Propagation x.grad # tensor([ 0., 4., 8., 12.]) # test x.grad == 4 * x # tensor([True, True, True, True]) x.grad.zero_() # clear previous cumulated result Backward for Non-Scalar Variables Invoking backward on a non-scalar elicits an error unless we tell PyTorch how to reduce the object to a scalar.\nx.grad.zero_() y = x * x y.backward(gradient=torch.ones(len(y))) # Faster: y.sum().backward() x.grad Detaching Computation We want to focus on the direct influence of x on z rather than the influence conveyed via y\nx.grad.zero_() y = x * x u = y.detach() # u become scalar z = u * x z.sum().backward() x.grad == u # tensor([True, True, True, True]) we can calculate the gradient of y with respect to x\nx.grad.zero_() y.sum().backward() x.grad == 2 * x # tensor([True, True, True, True]) Gradients and Python Control Flow We can still get gradient through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls).\ndef f(a): b = a * 2 while b.norm() \u0026lt; 1000: b = b * 2 if b.sum() \u0026gt; 0: c = b else: c = 100 * b return c a = torch.randn(size=(), requires_grad=True) # empty size for specific result d = f(a) d.backward() a.grad == d / a # true ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b88517776950b075f409bed8f6bee334","permalink":"https://x12hengyu.github.io/notes/d2l/3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/3/","section":"notes","summary":"Theories Computation Graph\nForward propagation\nMemory Complexity: O(1) Drawback: Gradient calculation for every layer, high time complexity Back propagation\nTime Complexity: O(N), Memory Complexity: O(N) Example\nLeft: Forward Propagation, Right, Back Propagation","tags":null,"title":"3. Automatic Differentiation","type":"book"},{"authors":null,"categories":null,"content":"e.g.\ndelimiter $$ drop procedure if exists getStudent; create procedure getStudent(IN studentName VARCHAR(255)) begin select * from STUDENT where sname = studentName; end $$ delimiter ; delimiter $$ Drop procedure if exists updateStudentTable; create procedure updateStudentTable() begin update student set age =22 where sname =\u0026#39;Edward Baker\u0026#39;; update student set major = \u0026#39;Architecture\u0026#39; where sname = \u0026#39;Charles Harris\u0026#39;; end $$ delimiter ; DATABASE API\nODBC (Open Database Connectivity) works with C, C++, C#, and Visual Basic\nJDBC (Java Database Connectivity) works with Java\nJDBC STEPS\nImporting Packages Registering the JDBC Drivers Opening a Connection to a Database Creating a Statement Object Executing a Query and Returning a Result Set Object Processing the Result Set Closing the Result Set and Statement Objects Closing the Connection ","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"dac4cfdcac74ba4bc0f59c2bb76602d3","permalink":"https://x12hengyu.github.io/notes/564db/4/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/4/","section":"notes","summary":"e.g.\ndelimiter $$ drop procedure if exists getStudent; create procedure getStudent(IN studentName VARCHAR(255)) begin select * from STUDENT where sname = studentName; end $$ delimiter ; delimiter $$ Drop procedure if exists updateStudentTable; create procedure updateStudentTable() begin update student set age =22 where sname ='Edward Baker'; update student set major = 'Architecture' where sname = 'Charles Harris'; end $$ delimiter ; DATABASE API","tags":null,"title":"PROCEDURES and FUNCTIONS","type":"book"},{"authors":null,"categories":null,"content":"Linear Model General Model: $\\hat{y} = w_1 x_1 + … + w_d x_d + b.$ Concise Model: $\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$ Linear regression can be regard as a single layer NN. Loss Model Squared error\n$$ l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2. $$\nLosses on the training set\n$$ L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2 $$\nMinimize the total loss across all training examples\n$$ \\mathbf{w}^, b^ = \\operatorname*{argmin}_{\\mathbf{w}, b}\\ L(\\mathbf{w}, b) $$\nAnalytic Solution\n$$ \\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y} $$\nMinibatch Stochastic Gradient Descent Problem trying to resolve: No Analytic Solution Initialize the values of the model parameters, typically at random; iteratively sample random Minibatches from the data, updating the parameters in the direction of the negative gradient. $$ (\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}t} \\partial{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b) $$\nHyperparameter: Tunable parameters that are not updated in the training loop $\\eta$ : Learning Rate, step size of gradient descent $|\\mathcal{B}|$ : Batch Size, we random choose $|\\mathcal{B}|$ samples and average the loss to approximate the general training loss. Here, $|\\mathcal{B}|$ gets smaller, the loss will be less accurate, but faster. However, for NN, small batch size is better for generalization. Hyperparameter Tuning: Choosing Hyperparameters Implementation with PyTorch from Scratch (Maybe) Generating the Dataset\ndef synthetic_data(w, b, num_examples): X = torch.normal(0, 1, (num_examples, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) # noise return X, y.reshape((-1, 1)) # W = [2, -3.4], b = 4.2, Y = Wx + b true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) Generating Mini Batch\ndef data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # shuffle indices for random access for i in range(0, num_examples, batch_size): # min handles i + batch_size exceed example len batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] # iterator batch_size = 10 for X, y in data_iter(batch_size, features, labels): # print(X, \u0026#39;\\n\u0026#39;, y) break Initialize Model\nw = torch.normal(0, 0.01, size=(2,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) # Model def linreg(X, w, b): return torch.matmul(X, w) + b # Loss Function def squared_loss(y_hat, y): return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 # Optimization def sgd(params, lr, batch_size): # We do not need grad here with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() Train\nlr = 0.03 num_epochs = 3 # We browse all data 3 times net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # Minibatch loss l.sum().backward() sgd([w, b], lr, batch_size) # use gradiant update parameters with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;) Concise Implementation with PyTorch Use Pytorch Data Module and generate data\nfrom torch.utils import data from d2l import torch as d2l true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = d2l.synthetic_data(true_w, true_b, 1000) Read Dataset\ndef load_array(data_arrays, batch_size, is_train=True): dataset = data.TensorDataset(*data_arrays) # split data_arrays with * return data.DataLoader(dataset, batch_size, shuffle=is_train) batch_size = 10 data_iter = load_array((features, labels), batch_size) next(iter(data_iter)) Model\nfrom torch import nn # Linear Regression can be treat as a signle layer nn net = nn.Sequential(nn.Linear(2, 1)) # W and b net[0].weight.data.normal_(0, 0.01) net[0].bias.data.fill_(0) # Loss Function loss = nn.MSELoss() # SGD # net.parameters(): W and b in our case trainer = torch.optim.SGD(net.parameters(), lr=0.03) Train\nnum_epochs = 3 for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X) ,y) trainer.zero_grad() l.backward() trainer.step() # update l = loss(net(features), labels) print(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2b376ccc874226d04bcdef48ca268d30","permalink":"https://x12hengyu.github.io/notes/d2l/4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/4/","section":"notes","summary":"Linear Model General Model: $\\hat{y} = w_1 x_1 + … + w_d x_d + b.$ Concise Model: $\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$ Linear regression can be regard as a single layer NN.","tags":null,"title":"4. Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"# First go to the user home directory. $ cd ~ $ pwd /Users/yuxizheng # Edit the user .bash_pforile file. $ sudo vim .bash_profile # Add below content at the end of the .bash_profile file. Save and exit. # Add MySQL binary folder in the macOS system environment variable PATH. PATH=\u0026#34;/usr/local/mysql/bin:${PATH}\u0026#34; export PATH # Verify the .bash_profile has been changed as required. $ cat .bash_profile # Make the above changes take effect in macOS. $ source .bash_profile # Now you can run mysql command directly in terminal to connect to the MySQL database server. $ mysql -u root -p # login # /usr/local/mysql/bin/mysql -u root -p mysql -u root -p mysql --local-infile=1 -u root -p A1b2C3d4\u0026amp; # set password SET PASSWORD = PASSWORD(\u0026#39;[newpassword]\u0026#39;); SET PASSWORD = PASSWORD(\u0026#39;[1234567890]\u0026#39;); SET GLOBAL local_infile = true; # show databases show databases; # select database use \u0026lt;db\u0026gt; # list tables show tables; # show table describe \u0026lt;table\u0026gt;; # load data load data local infile \u0026#39;path to the csv file\u0026#39; into table \u0026lt;table\u0026gt; fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; # exit mysql exit ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access\nload data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/cs564/lab1/universitydb/Enroll.csv\u0026#39; into table Enroll fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\r\\n\u0026#39;; // 13 select s.sname from student s where s.snum in ( select e1.snum from enroll e1, enroll e2, class c1, class c2 where c1.cname != c2.cname AND c1.meets_at = c2.meets_at AND e1.snum = e2.snum AND e1.cname = c1.cname AND e2.cname = c2.cname); // 20 select s1.age, s1.level from student s1 group by s1.age, s1.level having s1.level in ( select s2.level from student s2 where s1.age = s2.age group by s2.age, s2.level having count(*) \u0026gt;= ALL( select count(*) from student s3 where s2.age = s3.age group by s3.age, s3.level)); // 17 select f.fname, count(c.cname) AS num from class c, faculty f where c.fid = f.fid group by f.fname having f.fname NOT IN (select f1.fname from class c1, faculty f1 where c1.fid = f1.fid and c1.cname NOT IN (select c2.cname from class c2 where c2.room = \u0026#39;R128\u0026#39;)); CREATE TABLE assets ( symbol VARCHAR(50), date DATE, open DOUBLE, high DOUBLE, low DOUBLE, close DOUBLE, volume DOUBLE, PRIMARY KEY (symbol, date)); load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/commo_merged.csv\u0026#39; into table assets fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/stock_merged.csv\u0026#39; into table assets fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/crypto_merged.csv\u0026#39; into table assets fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; CREATE TABLE commo ( symbol VARCHAR(50), name VARCHAR(50), PRIMARY KEY (symbol)); load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/_metadata_commo.csv\u0026#39; into table commo fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; CREATE TABLE stock ( symbol VARCHAR(50), name VARCHAR(50), country VARCHAR(50), continent VARCHAR(50), PRIMARY KEY (symbol)); load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/_metadata_stock.csv\u0026#39; into table stock fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; CREATE TABLE crypto ( symbol VARCHAR(50), name VARCHAR(50), PRIMARY KEY (symbol)); load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/_metadata_crypto.csv\u0026#39; into table crypto fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; CREATE TABLE login ( user VARCHAR(50), password VARCHAR(50), PRIMARY KEY (user, password)); CREATE TABLE watchlist ( user VARCHAR(50), symbol VARCHAR(50), name VARCHAR(50), type VARCHAR(50), PRIMARY KEY (user, symbol)); CREATE TABLE currency ( name VARCHAR(50), symbol VARCHAR(50), date DATE, price VARCHAR(50), PRIMARY KEY (symbol, date)); load data local infile \u0026#39;/Users/yuxizheng/xizheng/2022-SUMMER/CS564-Final-Project/dataset/exchangeRate_merged.csv\u0026#39; into table currency fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; CREATE TABLE country ( symbol VARCHAR(50), name VARCHAR(50), continent VARCHAR(50), PRIMARY KEY (symbol)); SELECT a.symbol, AVG(a.close) FROM assets a, commo c Where c.symbol = a.symbol AND a.symbol IN ( SELECT DISTINCT symbol FROM assets WHERE date BETWEEN \u0026#39;2005-01-01\u0026#39; AND \u0026#39;2005-12-31\u0026#39;) GROUP BY a.symbol ORDER BY AVG(a.close) ASC LIMIT 5; SELECT c.symbol, COUNT(a.date) FROM assets a, commo c Where c.symbol = a.symbol GROUP BY c.symbol ORDER BY COUNT(a.date) DESC LIMIT 10; mysqldump -u root -p 564project \u0026gt; ./sql-data-backup.sql\n","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"c35e24eb535559d3b64663c401d3e473","permalink":"https://x12hengyu.github.io/notes/564db/5/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/5/","section":"notes","summary":"# First go to the user home directory. $ cd ~ $ pwd /Users/yuxizheng # Edit the user .bash_pforile file. $ sudo vim .bash_profile # Add below content at the end of the .","tags":null,"title":"mySQL","type":"book"},{"authors":null,"categories":null,"content":"Theories All probabilities are nonnegative. We can then transform these values so that they add up to 1 by dividing each by their sum. This process is called normalization.\n$$ \\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\text{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)} $$\nCross Entropy: compare difference between two probability\n$$ H(p, q) = \\sum_j - p_i \\log (q_i) $$\nUse Cross Entropy as Loss\n$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j = - \\log \\hat{y}_y $$\n$$ \\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j $$\nLoss Functions L2 Loss\n$$ l(y, y’) = \\frac{1}{2}(y - y’)^2 $$\nSometimes we do not expect large gradient to update parameter when our predicted value is far from the expected value L1 Loss\n$$ l(y, y’) = |y - y’| $$\nNon-derivative at 0, strong change between -1 and 1 around 0, not stable at the end More stable, provide stable update through training process Huber’s Robust Loss\n$$ l(y, y’) = \\begin{cases} |y - y’| - \\frac12, \u0026amp; \\text{if $|y - y’|\u0026gt;1$}.\\ \\frac{1}{2}(y - y’)^2, \u0026amp; \\text{otherwise}. \\end{cases} $$\nGradient is y = x from -1 to 1 Image Classification Dataset (Fashion-MNIST) import\n%matplotlib inline import torch import torchvision from torch.utils import data from torchvision import transforms from d2l import torch as d2l d2l.use_svg_display() Load to PyTorch\ntrans = transforms.ToTensor() # transform from image to tensor type mnist_train = torchvision.datasets.FashionMNIST( root=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST( root=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True) len(mnist_train), len(mnist_test) # (60000, 10000) mnist_train[0][0].shape # torch.Size([1, 28, 28]) # Black and white, first channel is 1 Visualization\ndef get_fashion_mnist_labels(labels): text_labels = [\u0026#39;t-shirt\u0026#39;, \u0026#39;trouser\u0026#39;, \u0026#39;pullover\u0026#39;, \u0026#39;dress\u0026#39;, \u0026#39;coat\u0026#39;, \u0026#39;sandal\u0026#39;, \u0026#39;shirt\u0026#39;, \u0026#39;sneaker\u0026#39;, \u0026#39;bag\u0026#39;, \u0026#39;ankle boot\u0026#39;] return [text_labels[int(i)] for i in labels] def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): figsize = (num_cols * scale, num_rows * scale) _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize) axes = axes.flatten() for i, (ax, img) in enumerate(zip(axes, imgs)): if torch.is_tensor(img): # image tensor ax.imshow(img.numpy()) else: # PIL ax.imshow(img) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) if titles: ax.set_title(titles[i]) return axes X, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y)); Read small batch data\nbatch_size = 256 def get_dataloader_workers(): \u0026#34;\u0026#34;\u0026#34;use four progresses\u0026#34;\u0026#34;\u0026#34; return 4 train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()) timer = d2l.Timer() for X, y in train_iter: continue f\u0026#39;{timer.stop():.2f} sec\u0026#39; Integral components\ndef load_data_fashion_mnist(batch_size, resize=None): trans = [transforms.ToTensor()] if resize: trans.insert(0, transforms.Resize(resize)) trans = transforms.Compose(trans) mnist_train = torchvision.datasets.FashionMNIST( root=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST( root=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True) return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers())) Softmax Regression Implementation from Scratch from IPython import display from mxnet import autograd, gluon, np, npx from d2l import mxnet as d2l npx.set_np() batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) Initialize\nnum_inputs = 784 # vector size from 28 * 28 to 784 num_outputs = 10 # output dimension W = np.random.normal(0, 0.01, size = (num_inputs, num_outputs), requires_grad=True) b = np.zeros(num_outputs, requires_grad=True) Softmax\ndef softmax(X): X_exp = np.exp(X) partition = X_exp.sum(1, keepdims=True) return X_exp / partition # broadcast # test X = np.random.normal(0, 1, (2, 5)) X_prob = softmax(X) X_prob, X_prob.sum(1) # (array([[0.22376052, 0.06659239, 0.06583703, 0.29964197, 0.3441681 ], # [0.63209665, 0.03179282, 0.194987 , 0.09209415, 0.04902935]]), # array([1. , 0.99999994])) Define Model\ndef net(X): # X.reshape((-1, W.shape[0])) reshapes X to batch_size (256) * vector (784) return softmax(np.dot(X.reshape((-1, W.shape[0])), W) + b) Define Cross Entropy Function\ny = np.array([0, 2]) # correct classfication label indices y_hat = np.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y_hat[[0, 1], y] # get row 0, 1, indices are 0, 2 from y # array([0.1, 0.5]) def cross_entropy(y_hat, y): return - np.log(y_hat[range(len(y_hat)), y]) cross_entropy(y_hat, y) Accuracy\ndef accuracy(y_hat, y): if len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1: y_hat = y_hat.argmax(axis=1) # …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a26cafc79031bb9d4a330dd1026391cd","permalink":"https://x12hengyu.github.io/notes/d2l/5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/5/","section":"notes","summary":"Theories All probabilities are nonnegative. We can then transform these values so that they add up to 1 by dividing each by their sum. This process is called normalization.\n$$ \\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\text{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)} $$","tags":null,"title":"5. Softmax Regression","type":"book"},{"authors":null,"categories":null,"content":"DISK\nA disk with Sector size 512 bytes ­2000 tracks per surface ­50 sectors per track ­5 double-sided platters ­Average seek time: 10msec 5400 revolutions per minute (rpm)\nCapacity of track: Sector size * Sectors 512 * 50 = 25600 bytes = 25 KB Capacity of surface: Capacity of track * surfaces 25KB * 2000 = 50,000 KB Capacity of disk: Capacity of surface * platters 50,000 KB * 5 * 2 = 500,000 KB Cylinders: Cylinders = Track on each platter 2000 Valid block size: Integer multiplication of sector size 256: No, 256 = 0.5 * 512 2048: Yes, 2048 = 512 * 4 Maximum rotational delay: Maximum rotational delay = time required for one complete rotation = 60 * 1 / rpm 60 * 1/5400 = 0.011 seconds Transfer rate transfer rate = track size / time required for one revolution 25KB / 0.011 seconds = 2272 KB/sec BUFFER MANAGEMENT\nPin count: number of users of the page Pin a page: indicate that the page is in use (pin count \u0026gt; 0)­ Unpin a page: release the page, and also indicate whether the page is dirtied (pin count = 0) Dirty bit: indicates whether the page has been modified and the changes need to be propagated to the disk LEAST RECENTLY USED (LRU)\nMOST RECENTLY USED (MRU)\nRECORD ORGANIZATION\nFile organization\nHeap file­ As doubly-linked list Using page directory Page organization­\nFor fixed-length records Packed­ Unpacked For variable-length records B+ TREE\nAdvantages:\nautomatically reorganizes itself with small, local, changes, in the face of insertions and deletions. reorganization of entire file is not required to maintain performance Disadvantages:\nextra insertion and deletion overhead, space overhead. General Properties:\nAll paths from root to leaf are of the same length Two types of nodes: index (internal) nodes and data (leaf) nodes. Each node is one disk page. Parameter d = the order (or the degree) Each node must have minimum 50% occupancy (except root) Each node and leaf has d ≤ m ≤ 2d keys VIEDO\nB+ Tree Basics 1\nB+ Trees Basics 2 (insertion)\nB+ Tree Basics 3\n","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"381a3d1ef260b7629dda8107578a4991","permalink":"https://x12hengyu.github.io/notes/564db/6/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/6/","section":"notes","summary":"DISK\nA disk with Sector size 512 bytes ­2000 tracks per surface ­50 sectors per track ­5 double-sided platters ­Average seek time: 10msec 5400 revolutions per minute (rpm)\nCapacity of track: Sector size * Sectors 512 * 50 = 25600 bytes = 25 KB Capacity of surface: Capacity of track * surfaces 25KB * 2000 = 50,000 KB Capacity of disk: Capacity of surface * platters 50,000 KB * 5 * 2 = 500,000 KB Cylinders: Cylinders = Track on each platter 2000 Valid block size: Integer multiplication of sector size 256: No, 256 = 0.","tags":null,"title":"DISK","type":"book"},{"authors":null,"categories":null,"content":"Perceptron Input X, Weight W, Bias b, Perceptron is Binary Classification\nTraining Algorithm\nSame as Gradient decent with batch size of 1, and use following loss function\nProblem: Perceptron does not solve XOR Function, It only creates linear cutoff line.\nThis Problem also led to the first AI Winter.\nMultilayer Perceptrons Single Hidden Layer\nWe use Softmax to handle multi-category classification\nMulti-Hidden Layers, Hyperparameters: Number of layers, layer size\nActivation Functions Sigmoid (squashing function): transforms inputs to outputs that lie on the interval (0, 1). It is smooth and differentiable approximation to a thresholding unit.\nTanh: Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs, transforming them into elements on the interval between -1 and 1\nReLU (*rectified linear unit):* Simple non linear transformation, the function is defined as the maximum of that element and 0. Fast and Easy to calculate.\nImplementation of Multilayer Perceptrons Initialize\nimport torch from torch import nn from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) num_inputs, num_outputs, num_hiddens = 784, 10, 256 W1 = nn.Parameter(torch.randn( num_inputs, num_hiddens, requires_grad=True) * 0.01) b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True)) W2 = nn.Parameter(torch.randn( num_hiddens, num_outputs, requires_grad=True) * 0.01) b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True)) params = [W1, b1, W2, b2] ReLU Activation\ndef relu(X): a = torch.zeros_like(X) # create same size matrix with all 0 return torch.max(X, a) Model\ndef net(X): X = X.reshape((-1, num_inputs)) H = relu(X @ W1 + b1) # 这里“@”matrix multiplication return (H @ W2 + b2) Loss Function\nloss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;) Training\nnum_epochs, lr = 10, 0.1 updater = torch.optim.SGD(params, lr=lr) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater) Concise Implementation of Multilayer Perceptrons net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights) batch_size, lr, num_epochs = 256, 0.1, 10 loss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;) trainer = torch.optim.SGD(net.parameters(), lr=lr) train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"379e46153fac0fb79f82551d02a2ccf7","permalink":"https://x12hengyu.github.io/notes/d2l/6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/6/","section":"notes","summary":"Perceptron Input X, Weight W, Bias b, Perceptron is Binary Classification\nTraining Algorithm\nSame as Gradient decent with batch size of 1, and use following loss function\nProblem: Perceptron does not solve XOR Function, It only creates linear cutoff line.","tags":null,"title":"6. MLP: Multilayer Perceptrons","type":"book"},{"authors":null,"categories":null,"content":"Static Hashing A hash index is a collection of buckets N = number of buckets h(k) mod N = bucket in which the data entry belongs Equality search Apply the hash function on the search key value to locate the appropriate bucket Search through the primary page (and possibly overflow pages if they exist) to find the matching record(s) Deletion Find the appropriate bucket, delete the record Possibly delete the overflow page Insertion Find the appropriate bucket, insert the record If there is no space, create a new overflow page Dynamic Hashing Good for database that grows and shrinks in size Allows the hash function to be modified dynamically If local depth = Global depth We need to double the size of table, add new bucket, redistribute data, update pointers If local depth \u0026lt; global depth Add new bucket, redistribute data, update pointers Benefits of extendable hashing: Hash performance does not degrade with growth of file Minimal space overhead Disadvantages of extendable hashing: Extra level of indirection to find desired record Bucket address table may itself become very big (larger than memory) Cannot allocate very large contiguous areas on disk either Solution: B+ tree structure to locate desired record in bucket address table Changing size of bucket address table is an expensive operation Sort Internal Sorting (data is in memory at all time while the sorting is in progress) Heapsort, Quicksort External Sorting (data is stored on disk and only small chunks are loaded inside the memory) 2-way merge sort, Multiway Mergesort Choice of internal sort algorithm may matter Quicksort: Quick!­ Replacement sort: slower, longer runs Clustered B+ tree is good for sorting Unclustered tree is usually very bad Clustered B+ tree: The search key order corresponds to the order in which the data records are stored. 2-WAY EXTERNAL MERGE SORT\nB = 2 N = number of pages Number of passes = $\\lceil \\log_2N \\rceil + 1$ Total Cost = $2N(\\lceil \\log_2N \\rceil + 1)$ GENERAL (MULTI-WAY) EXTERNAL MERGE-SORT\nB: number of available buffer pages N: number of pages in R Number of passes = $\\lceil \\log_{B-1} \\lceil {N \\over B} \\rceil \\rceil + 1$ 1 pass to create initial sorted runs $\\lceil \\log_{B-1} \\lceil {N \\over B} \\rceil \\rceil$ to merge runs repeatedly Number of I/Os per pass = 2N Hence, total cost = $2N (\\lceil \\log_{B-1} \\lceil {N \\over B} \\rceil \\rceil + 1)$ ","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"5a402fa478d20a399885962d0023a178","permalink":"https://x12hengyu.github.io/notes/564db/7/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/7/","section":"notes","summary":"Static Hashing A hash index is a collection of buckets N = number of buckets h(k) mod N = bucket in which the data entry belongs Equality search Apply the hash function on the search key value to locate the appropriate bucket Search through the primary page (and possibly overflow pages if they exist) to find the matching record(s) Deletion Find the appropriate bucket, delete the record Possibly delete the overflow page Insertion Find the appropriate bucket, insert the record If there is no space, create a new overflow page Dynamic Hashing Good for database that grows and shrinks in size Allows the hash function to be modified dynamically If local depth = Global depth We need to double the size of table, add new bucket, redistribute data, update pointers If local depth \u003c global depth Add new bucket, redistribute data, update pointers Benefits of extendable hashing: Hash performance does not degrade with growth of file Minimal space overhead Disadvantages of extendable hashing: Extra level of indirection to find desired record Bucket address table may itself become very big (larger than memory) Cannot allocate very large contiguous areas on disk either Solution: B+ tree structure to locate desired record in bucket address table Changing size of bucket address table is an expensive operation Sort Internal Sorting (data is in memory at all time while the sorting is in progress) Heapsort, Quicksort External Sorting (data is stored on disk and only small chunks are loaded inside the memory) 2-way merge sort, Multiway Mergesort Choice of internal sort algorithm may matter Quicksort: Quick!","tags":null,"title":"Hash Sorting","type":"book"},{"authors":null,"categories":null,"content":"Training Error vs Generalization Error Training Error: the error on the training dataset\nGeneralization Error: the true error on the new dataset / underlying population\nValidation Dataset vs Test Dataset That the “validation dataset” is predominately used to describe the evaluation of models when tuning hyperparameters and data preparation, and the “test dataset” is predominately used to describe the evaluation of a final tuned model when comparing it to other final models. Validation Dataset use multiple time to test our model not in training, reflect whether hyperparameters are good Test Dataset use once not use for hyperparameter We don’t usually have the “real” test dataset!!! But we usually use validation dataset as test dataset. However, we must remember the difference. And this will results big consequence to the model result. K-Fold Cross Validation our dataset is not big enough\ncut dataset into k blocks\nfor i : k, use block i as testing dataset, training on the rest\ncalculate mean of k errors\nMy Implementation from Statistic Class using R\nk_fold_fit_models \u0026lt;- function( holdout_idxs ) { # Split the mtcars data into train and the hold-out. train_data \u0026lt;- ToothGrowth[ -holdout_idxs, ]; leftout \u0026lt;- ToothGrowth[ holdout_idxs, ]; errors \u0026lt;- rep( NA, 2 ); # We\u0026#39;re training up to order 5. # Fit the linear model, then evaluate. m1 \u0026lt;- lm(len ~ 1 + supp + dose, train_data ); m1.pred \u0026lt;- predict( m1, leftout ); errors[1] \u0026lt;- mean( (m1.pred - leftout$len)^2 ); # Fit the intersection model, then evaluate. m2 \u0026lt;- lm(len ~ 1 + supp + dose + supp:dose, train_data ); m2.pred \u0026lt;- predict( m2, leftout ); errors[2] \u0026lt;- mean( (m2.pred - leftout$len)^2 ); return( errors ); } K = 5 n \u0026lt;- nrow(ToothGrowth); Kfolds \u0026lt;- split( sample(1:n, n,replace=FALSE), as.factor(1:K)); norder \u0026lt;- 2; Kfold_resids \u0026lt;- data.frame( \u0026#39;Order\u0026#39;=rep(c(\u0026#39;linear\u0026#39;, \u0026#39;intersection\u0026#39;), each=K), \u0026#39;Fold\u0026#39;=rep(1:K, norder ), \u0026#39;Error\u0026#39;=rep(NA, K*norder) ); for (k in 1:K ) { heldout_idxs \u0026lt;- Kfolds[[k]]; # The indices of the k-th hold-out set. # Now train the 5 different models and store their residuals. idx \u0026lt;- (Kfold_resids$Fold==k); Kfold_resids[idx, ]$Error \u0026lt;- k_fold_fit_models( heldout_idxs ); } KF_agg \u0026lt;- aggregate(Error ~ Order, data=Kfold_resids, FUN=mean); KF_agg N-Fold Cross Validation Maximize our dataset, use\nREALLY EXPENSIVE\nOverfitting vs Underfitting Model Capacity The ability to fit various functions\nIt is hard for Low Capacity model to fit function\nVery complex model (high capacity) might fit all dataset and function, cause overfitting\nEstimate Model Capacity\nHard to estimate between different types of model, e.g. tree vs nn\nLinear NN (regression): d + 1\nMLP: (d+1) * m + (m + 1) * k\nVC Dimension Provide a theoretical support to evaluate model 2D MLP, VC Dimension = 3, (because XOR 4d need non linear solution) N-Dimension MLP: VC-Dimension = N + 1 Some MLP has VC-Dimension = $O(N \\log_2 N)$ We do not use VC Dimension in NN, not accurate, hard to evaluate Data Complexity number of dataset element number of a single individual of dataset time, space structure data diversity Simulation 4.4. 模型选择、欠拟合和过拟合 - 动手学深度学习 2.0.0 documentation\nhttps://zh-v2.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id10\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c66453edd226f5671c671e43fa68fa00","permalink":"https://x12hengyu.github.io/notes/d2l/7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/d2l/7/","section":"notes","summary":"Training Error vs Generalization Error Training Error: the error on the training dataset\nGeneralization Error: the true error on the new dataset / underlying population\nValidation Dataset vs Test Dataset That the “validation dataset” is predominately used to describe the evaluation of models when tuning hyperparameters and data preparation, and the “test dataset” is predominately used to describe the evaluation of a final tuned model when comparing it to other final models.","tags":null,"title":"7. Model Selection, Overfitting, Underfitting","type":"book"},{"authors":null,"categories":null,"content":"Selection Projection Join nested loop\nCost: T(R) * T(S) (e.g: 100,000 x 40,000 = 4 x 10e9 I/O\nAssuming that each I/O takes 10ms -\u0026gt; this will take about 11,111 hours\nindex nested loop\nblock-based nested loop join\nCost = Outer table pages + ( Outer table pages / blocksize ) * inner table pages\nblock index nested loop\nsort-merge join\nTotal cost: EMS(R)+ EMS(S) + merging_cost (∈[T(R)+T(S), T(R) *T(S)]) Merging cost: T(R)+T(S) is guaranteed in foreign key join;\nhash join\nGENERAL JOIN CONDITIONS\nMultiple equalities­\ne.g. R.a = S.b ∧ R.c = S.d Hash Join works fine: hash R on (a, c) and S on (b, d)­ Sort Merge Join works fine: sort S on (a, c) and S on (b, d) ­Index Nested Loop Join: use (build, if needed) a matching index on S Inequalities\n­e.g. R.a \u0026gt; S.b ­Hash Join and Sort Merge Join not applicable­ Index Nested Loop Join: use (build, if needed) a B+tree index on S Block Nested Loop Join might often be among the best choices ­Inequality checks might lead to large outputs ","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"dc28395c13dedec56af7abf6eeba8a82","permalink":"https://x12hengyu.github.io/notes/564db/8/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/8/","section":"notes","summary":"Selection Projection Join nested loop\nCost: T(R) * T(S) (e.g: 100,000 x 40,000 = 4 x 10e9 I/O\nAssuming that each I/O takes 10ms -\u003e this will take about 11,111 hours","tags":null,"title":"Selection, Projection, Join Algos","type":"book"},{"authors":null,"categories":null,"content":"DB STATISTICS\nNumber of tuples (cardinality) T(R) Indexes, number of keys in the index V(R,a) Number of physical pages B(R) Min value, Max value, V(R,a) CARDINALITY AND COST ESTIMATION\nfile scan - B(R) selection - B(R) * selectivity nested loop - B(R) + T(R)* B(S) Index nested loop If index on S is clustered (default): B(R) + T(R)B(S)/V(S,a) If index on S is unclustered: B(R) + T(R)T(S)/V(S,a) block based nested loop - B(S) + B(R)*B(S)/M sort merge - 3B(R)+3B(S) hash join - 3B(R) + 3B(S) SELECTIVITY FACTOR\nRULE OF THUMB - If selectivities are unknown, then selectivity factor = 1/10\nCondition is A = c (value selection on R)\nSelectivity = 1 / V(R,A)\nCondition is A \u0026lt; c (range selection on R)\nSelectivity = (c - Low(R, A))/(High(R,A) - Low(R,A))\nCondition is A = B ($R \\bowtie_{A=B} S$)\nSelectivity = 1 / max(V(R,A),V(S,A))\nIn general:\n$T(R \\bowtie_{A=B} S) = T(R)T(S) / \\max(V(R,A), V(S,B))$\nEQUIVALENCE RULES\n1. Conjunctive selection operations can be deconstructed into a sequence of individual selections.\n$\\sigma_{\\theta_1 \\wedge \\theta_2}(E) = \\sigma_{\\theta_1}(\\sigma_{\\theta_2}(E))$\n2. Selection operations are commutative.\n$\\sigma_{\\theta_1}(\\sigma_{\\theta_2}(E)) = \\sigma_{\\theta_2}(\\sigma_{\\theta_1}(E))$\n3. Only the last in a sequence of projection operations is needed, the others can be omitted.\n4. Selections can be combined with Cartesian products and theta joins.\n$\\sigma_{\\theta}(E_1 \\times E_2) = E1 \\Join_{\\theta} E_2$\n$\\sigma_{\\theta_1}(E_1 \\Join_{\\theta_2} E_2) = E1 \\Join_{\\theta1 \\wedge \\theta2} E_2$\n5. Theta-join operations (and natural joins) are commutative.\n$E_1 \\Join_\\theta E_2 = E_2 \\Join_\\theta E_1$\n6. (a) Natural join operations are associative:\n$(E_1 \\Join E_2) \\Join E_3 = E_1 \\Join (E_2 \\Join E_3)$\n**(b) Theta joins are associative in the following manner:** $(E_1 \\Join_{\\theta_1} E_2) \\Join_{\\theta2 \\wedge \\theta3} E_3 = E_1 \\Join_{\\theta1 \\wedge \\theta3} (E_2 \\Join_{\\theta_2} E_3)$\n7. The selection operation distributes over the theta join operation under the following two conditions:\n(a) When all the attributes in θ0 involve only the attributes of one of the expressions (E1) being joined.\n(b) When θ1 involves only the attributes of E1 and θ2 involves only the attributes of E2.\n","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"0d93ae900ef61fe45d54570426a4f8f2","permalink":"https://x12hengyu.github.io/notes/564db/9/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/notes/564db/9/","section":"notes","summary":"DB STATISTICS\nNumber of tuples (cardinality) T(R) Indexes, number of keys in the index V(R,a) Number of physical pages B(R) Min value, Max value, V(R,a) CARDINALITY AND COST ESTIMATION\nfile scan - B(R) selection - B(R) * selectivity nested loop - B(R) + T(R)* B(S) Index nested loop If index on S is clustered (default): B(R) + T(R)B(S)/V(S,a) If index on S is unclustered: B(R) + T(R)T(S)/V(S,a) block based nested loop - B(S) + B(R)*B(S)/M sort merge - 3B(R)+3B(S) hash join - 3B(R) + 3B(S) SELECTIVITY FACTOR","tags":null,"title":"QUERY PERFORMANCE","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://x12hengyu.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"1534b71b5bdd93fc6045c9c42b3b093a","permalink":"https://x12hengyu.github.io/project/22fa-639-asl-system/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/project/22fa-639-asl-system/","section":"project","summary":"We use computer vision techniques to recognize ASL hand gestures. Achieve real-time translation with OpenCV.","tags":null,"title":"Sign Language Detection System","type":"project"},{"authors":null,"categories":null,"content":"","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"bff79880cc7f73e2755a1611f7f12399","permalink":"https://x12hengyu.github.io/project/639-tracking/","publishdate":"2022-11-28T00:00:00Z","relpermalink":"/project/639-tracking/","section":"project","summary":"A vision system that tracks the location of an object across video frames. We are going to compute the optival flow of given video frames and using its intensity histogram to track the object.","tags":null,"title":"Object Tracking based on Optical Flow","type":"project"},{"authors":null,"categories":null,"content":"","date":1669334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669334400,"objectID":"1c2a2e199e18a9e5f8138d0593d13b1e","permalink":"https://x12hengyu.github.io/project/639-img-stitch/","publishdate":"2022-11-25T00:00:00Z","relpermalink":"/project/639-img-stitch/","section":"project","summary":"An “Image Mosaicking App” that stitches a collection of photos into a mosaic. We use Homography, Correspondence, SIFT, RANSAC (RANdom Sampling And Consensus) to stitch images together.","tags":null,"title":"Image Mosaicking App","type":"project"},{"authors":null,"categories":null,"content":"","date":1668902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668902400,"objectID":"7899799efd5e108ac2e4d4cdee0f8dc0","permalink":"https://x12hengyu.github.io/project/639-refocusing/","publishdate":"2022-11-20T00:00:00Z","relpermalink":"/project/639-refocusing/","section":"project","summary":"An application that allows a user to refocus a scene. Our application use modified Laplacian as the focus measure and find the best match for selected refocus scene.","tags":null,"title":"Refocusing","type":"project"},{"authors":null,"categories":null,"content":"","date":1661558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661558400,"objectID":"fcdd45aea29d559a214870fee5dd65ea","permalink":"https://x12hengyu.github.io/project/564-dmbs-of-finance/","publishdate":"2022-08-27T00:00:00Z","relpermalink":"/project/564-dmbs-of-finance/","section":"project","summary":"Full Stack Financial Application backed with MySQL database that allows users customize financial products and visualize into graphs.","tags":null,"title":"DBMS of Major Investment Financial Assets","type":"project"},{"authors":null,"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"0d23049556345152a6d571667ccc6aa3","permalink":"https://x12hengyu.github.io/project/21su-skin-cancer/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/project/21su-skin-cancer/","section":"project","summary":"CNN classification on HAM10000 skin cancer dataset with NLP chatbot and Tkinter UI","tags":null,"title":"Skin Cancer Classification Chatbot","type":"project"},{"authors":["Yujia Guo","Zijian Ye","Xizheng (Hugh) Yu","Yuze Zhao"],"categories":[],"content":"","date":1609615045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609615045,"objectID":"60f71de14b0d84ae0b619beb56e172cc","permalink":"https://x12hengyu.github.io/publication/skin-cancer-paper/","publishdate":"2021-11-05T13:17:25-06:00","relpermalink":"/publication/skin-cancer-paper/","section":"publication","summary":"Skin cancer, abnormal skin cell development, is a common and fatal type of cancer that occurs when skin is exposed to sunlight. Early diagnosis is important to prevent more serious consequences. Implementing a detection system would save more time for doctors and give patients efficient and low-cost diagnoses. In this paper, we built a skin cancer classification system based on Convoluted Neural Network (CNN) for seven majority skin cancers, and Natural Language Processing (NLP), for interaction with a human. We also implemented self-defined CNN, LeNet5, AlexNet, ResNet, VGG-16 in our system to compare their accuracy and discover reasons behind those output data. Finally, our self-defined CNN gets 0.8237 testing accuracy after training, LeNet5 results in 0.4857 testing accuracy, AlexNet produces 0.4715 testing accuracy, ResNet yields 0.8995 testing accuracy, and VGG-16 shown 0.7544 testing accuracy. The result indicates that ResNet-18 performs best through all models.","tags":[],"title":"CNN Implementation on Major Skin Cancer Types Classification and NLP Diagnose Robot System","type":"publication"},{"authors":["Xizheng (Hugh) Yu","吳恩達"],"categories":["Demo","教程"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://x12hengyu.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart.\n","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"84a876ba789bb7232be8d9ed2487fd98","permalink":"https://x12hengyu.github.io/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://x12hengyu.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Xizheng (Hugh) Yu"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post’s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://x12hengyu.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://x12hengyu.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]